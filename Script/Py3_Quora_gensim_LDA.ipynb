{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim,os\n",
    "import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../Data/train_clean.csv',encoding='utf-8')\n",
    "qid_stack = df_train['qid1'].append(df_train['qid2'])\n",
    "df_train_naExc = df_train[~df_train.isnull().any(axis=1)]\n",
    "df_buf1 = df_train_naExc[['qid1','question1']]\n",
    "df_buf2 = df_train_naExc[['qid2','question2']]\n",
    "df_buf1.columns = ['qid','question']\n",
    "df_buf2.columns = ['qid','question']\n",
    "df_stack = df_buf1.append(df_buf2)\n",
    "df_stack_dupExc = df_stack.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_buf = df_stack_dupExc['question']#.head(1000)#.iloc[5000:7000]#head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537931"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stack_dupExc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_LDA_buf = test_buf.tolist()\n",
    "doc_set = test_LDA_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile sample documents into a list\n",
    "# doc_buf = df_stack_dupExc['question']\n",
    "# doc_buf = doc_buf.tolist()\n",
    "# doc_set = doc_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########\n",
    "######### Tokenize the doc\n",
    "#########\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "###\n",
    "def tokenize_doc(new_corpus):\n",
    "    new_corpus\n",
    "    texts = []\n",
    "    ###\n",
    "    ### Progressing bar setting\n",
    "    totall_size = len(new_corpus)\n",
    "    counter = 0\n",
    "    progress = 0\n",
    "    # loop through document list\n",
    "    for i in new_corpus:    \n",
    "        ### Monitor the progress\n",
    "        if progress%(1000) ==0:\n",
    "            bar = \"[\" + int(progress/1000)*\"|\"+\">\"+ (10-int(progress/1000))*\"-\" + str((progress/100)) + \"%\" \"]\"\n",
    "            print(bar)\n",
    "        #### count\n",
    "        counter += 1\n",
    "        progress = int(10000*(counter/totall_size))\n",
    "\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        \n",
    "        # stem tokens\n",
    "#         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        stemmed_tokens = []\n",
    "        for i in stopped_tokens:\n",
    "            try:\n",
    "                stemmed_tokens = stemmed_tokens + [p_stemmer.stem(i)]\n",
    "            except:\n",
    "                print(i)\n",
    "        # add tokens to list            \n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "######\n",
    "######  predict the topic\n",
    "######\n",
    "def topic_predict(new_corpus, model, dictionary):\n",
    "    tokenize_buf = tokenize_doc([new_corpus])\n",
    "    doc_bow = [dictionary.doc2bow(text) for text in tokenize_buf]\n",
    "    que_vec = [item for itemList in doc_bow for item in itemList]\n",
    "    topic_vec = ldamodel[que_vec]\n",
    "\n",
    "    word_count_array = np.empty((len(topic_vec), 2), dtype = np.object)\n",
    "    for i in range(len(topic_vec)):\n",
    "        word_count_array[i, 0] = topic_vec[i][0]\n",
    "        word_count_array[i, 1] = topic_vec[i][1]\n",
    "\n",
    "    idx = np.argsort(word_count_array[:, 1])\n",
    "    idx = idx[::-1]\n",
    "    word_count_array = word_count_array[idx]\n",
    "\n",
    "    final = []\n",
    "    final = ldamodel.print_topic(word_count_array[0, 0], len(word_count_array))#1)\n",
    "\n",
    "    question_topic = final.split('*') ## as format is like \"probability * topic\"\n",
    "    return question_topic\n",
    "    #return question_topic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Some word can not be  [stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]]\n",
    "##### Such like \"aed\"\n",
    "# new_corpus = doc_set\n",
    "# texts = []\n",
    "# # loop through document list\n",
    "# for i in new_corpus:    \n",
    "#     # clean and tokenize document string\n",
    "#     raw = i.lower()\n",
    "#     tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "#     # remove stop words from tokens\n",
    "#     stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "#     # stem tokens\n",
    "#     try:\n",
    "#         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "#     except:\n",
    "#         print(i)\n",
    "#     # add tokens to list\n",
    "#     texts.append(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "[>----------0.0%]\n",
      "aed\n",
      "aed\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "[|>---------10.0%]\n",
      "aed\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "[||>--------20.0%]\n",
      "aed\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "[|||>-------30.0%]\n",
      "aed\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "[||||>------40.0%]\n",
      "aed\n",
      "aed\n",
      "aed\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "[|||||>-----50.0%]\n",
      "aed\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "[||||||>----60.0%]\n",
      "aed\n",
      "aed\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "[|||||||>---70.0%]\n",
      "aed\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "[||||||||>--80.0%]\n",
      "aed\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n",
      "[|||||||||>-90.0%]\n"
     ]
    }
   ],
   "source": [
    "########## pre-processing ##########\n",
    "####################################\n",
    "    \n",
    "# list for tokenized documents in loop\n",
    "# texts = []\n",
    "\n",
    "# # loop through document list\n",
    "# for i in doc_set:\n",
    "    \n",
    "#     # clean and tokenize document string\n",
    "#     raw = i.lower()\n",
    "#     tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "#     # remove stop words from tokens\n",
    "#     stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "#     # stem tokens\n",
    "#     stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "#     # add tokens to list\n",
    "#     texts.append(stemmed_tokens)\n",
    "texts = tokenize_doc(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_buf = {\"tokenize_data\": texts, \n",
    "            \"dictionary\": dictionary, \n",
    "            \"encoding_corpus\": corpus, \n",
    "            \"lda_model\":ldamodel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# selfref_list = [1, 2, 3]\n",
    "# selfref_list.append(selfref_list)\n",
    "\n",
    "output = open('ldamodel_traing_data.pkl', 'wb')\n",
    "\n",
    "# Pickle dictionary using protocol 0.\n",
    "pickle.dump(dict_buf, output)\n",
    "\n",
    "# Pickle the list using the highest protocol available.\n",
    "#pickle.dump(selfref_list, output, -1)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pprint, pickle\n",
    "\n",
    "# pkl_file = open('ldamodel_traing_data.pkl', 'rb')\n",
    "\n",
    "# data2 = pickle.load(pkl_file)\n",
    "# # pprint.pprint(data2)\n",
    "\n",
    "# # data2 = pickle.load(pkl_file)\n",
    "# # pprint.pprint(data2)\n",
    "\n",
    "# pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the best age to teach a child how to swim \n",
      "['0.039', '\"differ\" + 0.034', '\"s\" + 0.018', '\"feel\" + 0.015', '\"human\" + 0.013', '\"like\" + 0.010', '\"10\" + 0.010', '\"type\" + 0.009', '\"one\" + 0.008', '\"anim\" + 0.008', '\"real\"']\n"
     ]
    }
   ],
   "source": [
    "pivot = 1000\n",
    "print(df_stack_dupExc['question'].iloc[pivot])\n",
    "print(topic_predict(df_stack_dupExc['question'].iloc[pivot], ldamodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Method to find separation of slits using fresnel biprism '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_LDA_buf[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.032',\n",
       " '\"best\" + 0.013',\n",
       " '\"india\" + 0.010',\n",
       " '\"s\" + 0.008',\n",
       " '\"can\" + 0.007',\n",
       " '\"get\" + 0.007',\n",
       " '\"chang\" + 0.006',\n",
       " '\"countri\" + 0.006',\n",
       " '\"movi\" + 0.004',\n",
       " '\"porn\" + 0.004',\n",
       " '\"us\"']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_predict(test_LDA_buf[10], ldamodel, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
