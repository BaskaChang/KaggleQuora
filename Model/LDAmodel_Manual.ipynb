{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim, os, pickle\n",
    "import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### take some data\n",
    "df_train = pd.read_csv('../../Data/train_clean.csv',encoding='utf-8')\n",
    "qid_stack = df_train['qid1'].append(df_train['qid2'])\n",
    "df_train_naExc = df_train[~df_train.isnull().any(axis=1)]\n",
    "df_buf1 = df_train_naExc[['qid1','question1']]\n",
    "df_buf2 = df_train_naExc[['qid2','question2']]\n",
    "df_buf1.columns = ['qid','question']\n",
    "df_buf2.columns = ['qid','question']\n",
    "df_stack = df_buf1.append(df_buf2)\n",
    "df_stack_dupExc = df_stack.drop_duplicates()\n",
    "test_buf = df_stack_dupExc['question']#.head(1000)#.iloc[5000:7000]#head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokenize_data', 'dictionary', 'encoding_corpus', 'lda_model'])\n"
     ]
    }
   ],
   "source": [
    "pkl_file = open('ldamodel_traingData_5300k.pkl', 'rb')\n",
    "ldamodel_dump = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print(ldamodel_dump.keys())\n",
    "##### for predict \n",
    "ldamodel = ldamodel_dump['lda_model']\n",
    "dictionary = ldamodel_dump['dictionary']\n",
    "##### for build the lda-model by yourself \n",
    "corpus = ldamodel_dump['encoding_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# some tools\n",
    "######### Tokenize the corpus\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "def tokenize_doc(new_corpus):\n",
    "    new_corpus\n",
    "    texts = []\n",
    "    ###\n",
    "    ### Progressing bar setting\n",
    "    totall_size = len(new_corpus)\n",
    "    counter = 0\n",
    "    progress = 0\n",
    "    # loop through document list\n",
    "    for i in new_corpus:    \n",
    "#         ### Monitor the progress\n",
    "#         if progress%(1000) ==0:\n",
    "#             bar = \"[\" + int(progress/1000)*\"|\"+\">\"+ (10-int(progress/1000))*\"-\" + str((progress/100)) + \"%\" \"]\"\n",
    "#             print(bar)\n",
    "#         #### count\n",
    "#         counter += 1\n",
    "#         progress = int(10000*(counter/totall_size))\n",
    "\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        \n",
    "        # stem tokens\n",
    "#         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        stemmed_tokens = []\n",
    "        for i in stopped_tokens:\n",
    "            try:\n",
    "                stemmed_tokens = stemmed_tokens + [p_stemmer.stem(i)]\n",
    "            except:\n",
    "                print(i)\n",
    "        # add tokens to list            \n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "######  predict the topic\n",
    "def topic_predict(new_corpus, model, dictionary):\n",
    "    tokenize_buf = tokenize_doc([new_corpus])\n",
    "    doc_bow = [dictionary.doc2bow(text) for text in tokenize_buf]\n",
    "    que_vec = [item for itemList in doc_bow for item in itemList]\n",
    "    topic_vec = ldamodel[que_vec]\n",
    "\n",
    "    word_count_array = np.empty((len(topic_vec), 2), dtype = np.object)\n",
    "    for i in range(len(topic_vec)):\n",
    "        word_count_array[i, 0] = topic_vec[i][0]\n",
    "        word_count_array[i, 1] = topic_vec[i][1]\n",
    "\n",
    "    idx = np.argsort(word_count_array[:, 1])\n",
    "    idx = idx[::-1]\n",
    "    word_count_array = word_count_array[idx]\n",
    "\n",
    "    final = []\n",
    "    final = model.print_topic(word_count_array[0, 0], len(word_count_array))#1)\n",
    "\n",
    "    question_topic = final.split('*') ## as format is like \"probability * topic\"\n",
    "    return question_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.025*\"year\" + 0.023*\"get\" + 0.020*\"engin\" + 0.018*\"can\"'), (1, '0.056*\"can\" + 0.027*\"use\" + 0.017*\"busi\" + 0.016*\"find\"'), (2, '0.033*\"like\" + 0.027*\"peopl\" + 0.020*\"feel\" + 0.019*\"indian\"'), (3, '0.118*\"best\" + 0.041*\"way\" + 0.023*\"book\" + 0.019*\"movi\"'), (4, '0.028*\"can\" + 0.025*\"life\" + 0.017*\"get\" + 0.016*\"first\"'), (5, '0.026*\"can\" + 0.020*\"s\" + 0.015*\"t\" + 0.011*\"world\"'), (6, '0.027*\"differ\" + 0.017*\"us\" + 0.015*\"s\" + 0.014*\"state\"'), (7, '0.026*\"work\" + 0.024*\"mean\" + 0.021*\"univers\" + 0.015*\"name\"'), (8, '0.046*\"make\" + 0.040*\"can\" + 0.039*\"t\" + 0.028*\"learn\"'), (9, '0.021*\"app\" + 0.020*\"can\" + 0.015*\"use\" + 0.013*\"human\"')]\n"
     ]
    }
   ],
   "source": [
    "###### check the topic stack\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can we acquire a positive morality \n",
      "['0.028', '\"can\" + 0.025', '\"life\" + 0.017', '\"get\" + 0.016', '\"first\" + 0.015', '\"quora\" + 0.014', '\"go\" + 0.014', '\"thing\" + 0.013', '\"someon\" + 0.013', '\"googl\" + 0.012', '\"know\"']\n"
     ]
    }
   ],
   "source": [
    "###### Predict a topic\n",
    "pivot = 500\n",
    "print(test_buf.iloc[pivot])\n",
    "print(topic_predict(test_buf.iloc[pivot], ldamodel, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting in 1000 pieces, including 537 corpus per piece\n"
     ]
    }
   ],
   "source": [
    "###### Let's change the model by yourself\n",
    "######***********************************\n",
    "###### setting\n",
    "num_topics = 5\n",
    "display_num_topic = 5\n",
    "## which data you want test\n",
    "test_pivot = 1000\n",
    "## How many piece\n",
    "num_piece = 1000\n",
    "######***********************************\n",
    "######  train by a sub-set\n",
    "### corpus splitting \n",
    "piece_len = int(len(corpus)/num_piece)\n",
    "sub_corpus_stack = []\n",
    "sub_corpus_stack = [corpus[x:x+piece_len] for x in range(0, len(corpus), piece_len)]\n",
    "###\n",
    "print(\"Splitting in \" + str(num_piece)+\" pieces\"+\", including \" + str(piece_len) +\" corpus per piece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### training a small LDA-model\n",
    "ldamodel_self = gensim.models.ldamodel.LdaModel(sub_corpus_stack[0], \n",
    "                                                num_topics=num_topics, \n",
    "                                                id2word = dictionary, \n",
    "                                                passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## Your LDA-Model ##############################\n",
      "The question: 'What is the best age to teach a child how to swim '\n",
      "\n",
      "The Token: ['best', 'age', 'teach', 'child', 'swim']\n",
      "\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.001', 'can', 'quora', 'look', 'life', 'porn', 'take', 'mean', 'stop', 'answer', 'question']\n",
      "['0.001', '\"can\" + 0.001', '\"quora\" + 0.001', '\"look\" + 0.001', '\"life\" + 0.000', '\"porn\" + 0.000', '\"take\" + 0.000', '\"mean\" + 0.000', '\"stop\" + 0.000', '\"answer\" + 0.000', '\"question\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "############################## My LDA-Model ##############################\n",
      "The question: 'What is the best age to teach a child how to swim '\n",
      "\n",
      "The match topic:\n",
      "['best']\n",
      "The topic set: ['0.118', 'best', 'way', 'book', 'movi', 'buy', 'ever', 's', 'good', 'game', 'place']\n",
      "['0.118', '\"best\" + 0.041', '\"way\" + 0.023', '\"book\" + 0.019', '\"movi\" + 0.019', '\"buy\" + 0.018', '\"ever\" + 0.017', '\"s\" + 0.014', '\"good\" + 0.013', '\"game\" + 0.012', '\"place\"']\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "###### check the topic stack of your lda\n",
    "##########################################\n",
    "print(30*\"#\" + \" Your LDA-Model \" + 30*\"#\")\n",
    "###### check the prediction of your lda\n",
    "print(\"The question: '\" + test_buf.iloc[test_pivot] +\"'\\n\")\n",
    "label_token = tokenize_doc([test_buf.iloc[test_pivot]])[0]\n",
    "predict_stack = [topic.split(\"+\")[0].replace('\"',\"\").replace(\" \",\"\") for topic in topic_predict(test_buf.iloc[test_pivot], ldamodel_self, dictionary)]\n",
    "figure_out_stack = [figure_out for figure_out in predict_stack if figure_out in label_token]    \n",
    "print(\"The Token: \" + str(label_token)+\"\\n\")\n",
    "print(\"The match topic:\")\n",
    "print(str(figure_out_stack))\n",
    "print(\"The topic set: \"+ str(predict_stack))\n",
    "print(topic_predict(test_buf.iloc[test_pivot], ldamodel_self, dictionary))\n",
    "print(90*\"-\")\n",
    "##########################################\n",
    "###### check the topic stack of huge-corpus lda as benchmark\n",
    "##########################################\n",
    "print(\"\\n\")\n",
    "###  the large corpus\n",
    "print(30*\"#\" + \" My LDA-Model \" + 30*\"#\")\n",
    "print(\"The question: '\" + test_buf.iloc[test_pivot] + \"'\\n\")\n",
    "predict_stack = [topic.split(\"+\")[0].replace('\"',\"\").replace(\" \",\"\") for topic in topic_predict(test_buf.iloc[test_pivot], ldamodel, dictionary)]\n",
    "figure_out = [figure_out for figure_out in predict_stack if figure_out in label_token]    \n",
    "print(\"The match topic:\")\n",
    "print(str(figure_out))\n",
    "print(\"The topic set: \"+ str(predict_stack))\n",
    "print(topic_predict(test_buf.iloc[test_pivot], ldamodel, dictionary))\n",
    "print(90*\"-\")\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### LDA-Model: Train Data - 1sub_corpus set###############\n",
      "The question: 'What is the best age to teach a child how to swim '\n",
      "\n",
      "The Token: ['best', 'age', 'teach', 'child', 'swim']\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.001', 'can', 'use', 'will', 'take', 't', 'war', 'love', 'find', 'differ', 'mani']\n",
      "['0.001', '\"can\" + 0.001', '\"use\" + 0.001', '\"will\" + 0.000', '\"take\" + 0.000', '\"t\" + 0.000', '\"war\" + 0.000', '\"love\" + 0.000', '\"find\" + 0.000', '\"differ\" + 0.000', '\"mani\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 2 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.002', 'math', 'relationship', 'develop', 'use', 'bodi', 'mobil', 'believ', 'hard', 'comput', 'ever']\n",
      "['0.002', '\"math\" + 0.001', '\"relationship\" + 0.001', '\"develop\" + 0.001', '\"use\" + 0.001', '\"bodi\" + 0.001', '\"mobil\" + 0.001', '\"believ\" + 0.001', '\"hard\" + 0.001', '\"comput\" + 0.001', '\"ever\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 3 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.001', 'app', 'hack', 'develop', 'laptop', 'public', 'applic', 'hip', 'stop', 'women', 'drug']\n",
      "['0.001', '\"app\" + 0.001', '\"hack\" + 0.001', '\"develop\" + 0.001', '\"laptop\" + 0.001', '\"public\" + 0.001', '\"applic\" + 0.001', '\"hip\" + 0.001', '\"stop\" + 0.001', '\"women\" + 0.001', '\"drug\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 4 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.002', 'app', 'develop', 'mobil', 'water', 'men', 'x', 'women', 'attract', 'center', 'purpos']\n",
      "['0.002', '\"app\" + 0.002', '\"develop\" + 0.002', '\"mobil\" + 0.001', '\"water\" + 0.001', '\"men\" + 0.001', '\"x\" + 0.001', '\"women\" + 0.001', '\"attract\" + 0.001', '\"center\" + 0.001', '\"purpos\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 5 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.002', 'app', 'hack', 'develop', 'attract', 'score', 'laptop', 'water', 'deal', 'purpos', 'x']\n",
      "['0.002', '\"app\" + 0.002', '\"hack\" + 0.001', '\"develop\" + 0.001', '\"attract\" + 0.001', '\"score\" + 0.001', '\"laptop\" + 0.001', '\"water\" + 0.001', '\"deal\" + 0.001', '\"purpos\" + 0.001', '\"x\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 6 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.003', 'mobil', 'hack', 'laptop', 'app', 'develop', 'drug', 'inertia', 'attract', 'success', 'men']\n",
      "['0.003', '\"mobil\" + 0.001', '\"hack\" + 0.001', '\"laptop\" + 0.001', '\"app\" + 0.001', '\"develop\" + 0.001', '\"drug\" + 0.001', '\"inertia\" + 0.001', '\"attract\" + 0.001', '\"success\" + 0.001', '\"men\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 7 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.002', 'x', 'public', 'app', 'attract', 'laptop', 'center', 'mobil', 'score', 'gram', 'chocol']\n",
      "['0.002', '\"x\" + 0.002', '\"public\" + 0.002', '\"app\" + 0.001', '\"attract\" + 0.001', '\"laptop\" + 0.001', '\"center\" + 0.001', '\"mobil\" + 0.001', '\"score\" + 0.001', '\"gram\" + 0.001', '\"chocol\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 8 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.003', 'app', 'water', 'model', 'million', 'self', 'laptop', 'men', 'develop', 'hack', 'benefit']\n",
      "['0.003', '\"app\" + 0.003', '\"water\" + 0.003', '\"model\" + 0.002', '\"million\" + 0.002', '\"self\" + 0.002', '\"laptop\" + 0.002', '\"men\" + 0.002', '\"develop\" + 0.002', '\"hack\" + 0.002', '\"benefit\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 9 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.004', 'laptop', 'app', 'develop', 'mobil', 'convert', 'water', 'men', 'actual', 'known', 'x']\n",
      "['0.004', '\"laptop\" + 0.003', '\"app\" + 0.002', '\"develop\" + 0.002', '\"mobil\" + 0.002', '\"convert\" + 0.002', '\"water\" + 0.002', '\"men\" + 0.002', '\"actual\" + 0.002', '\"known\" + 0.002', '\"x\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 10 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.003', 'app', 'water', 'men', 'x', 'near', 'fastest', 'given', 'actual', 'hack', 'area']\n",
      "['0.003', '\"app\" + 0.002', '\"water\" + 0.002', '\"men\" + 0.002', '\"x\" + 0.002', '\"near\" + 0.002', '\"fastest\" + 0.002', '\"given\" + 0.001', '\"actual\" + 0.001', '\"hack\" + 0.001', '\"area\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 11 sub_corpus set###############\n",
      "The match topic:\n",
      "[]\n",
      "The topic set: ['0.004', 'score', 'commun', 'app', 'water', 'fastest', 'given', 'actual', 'grid', 'mobil', 'kind']\n",
      "['0.004', '\"score\" + 0.003', '\"commun\" + 0.002', '\"app\" + 0.002', '\"water\" + 0.002', '\"fastest\" + 0.002', '\"given\" + 0.002', '\"actual\" + 0.002', '\"grid\" + 0.002', '\"mobil\" + 0.002', '\"kind\"']\n",
      "------------------------------------------------------------------------------------------\n",
      "############### LDA-Model: Train Data - 1000 sub_corpus set###############\n",
      "The match topic:\n",
      "['best']\n",
      "The topic set: ['0.118', 'best', 'way', 'book', 'movi', 'buy', 'ever', 's', 'good', 'game', 'place']\n",
      "['0.118', '\"best\" + 0.041', '\"way\" + 0.023', '\"book\" + 0.019', '\"movi\" + 0.019', '\"buy\" + 0.018', '\"ever\" + 0.017', '\"s\" + 0.014', '\"good\" + 0.013', '\"game\" + 0.012', '\"place\"']\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "############ The change from number of corpus\n",
    "###### Initialized LDA-model\n",
    "ldamodel_self = gensim.models.ldamodel.LdaModel(sub_corpus_stack[0], \n",
    "                                                num_topics=num_topics, \n",
    "                                                id2word = dictionary, \n",
    "                                                passes=20)\n",
    "### Monitor\n",
    "print(15*\"#\" + \" LDA-Model: Train Data - \" + str(1) + \"sub_corpus set\" + 15*\"#\")\n",
    "###### check the prediction of your lda\n",
    "print(\"The question: '\" + test_buf.iloc[test_pivot] +\"'\\n\")\n",
    "label_token = tokenize_doc([test_buf.iloc[test_pivot]])[0]\n",
    "print(\"The Token: \" + str(label_token))\n",
    "##### check the prediction\n",
    "predict_stack = [topic.split(\"+\")[0].replace('\"',\"\").replace(\" \",\"\") for topic in topic_predict(test_buf.iloc[test_pivot], ldamodel_self, dictionary)]\n",
    "figure_out = [figure_out for figure_out in predict_stack if figure_out in label_token]    \n",
    "print(\"The match topic:\")\n",
    "print(str(figure_out))\n",
    "print(\"The topic set: \"+ str(predict_stack))\n",
    "print(topic_predict(test_buf.iloc[test_pivot], ldamodel_self, dictionary))\n",
    "print(90*\"-\")\n",
    "###### increase the corpus in model\n",
    "\n",
    "for corpus_piece in range(1,11):\n",
    "    ldamodel_self.update(sub_corpus_stack[corpus_piece])\n",
    "    ### Monitor\n",
    "    print(15*\"#\" + \" LDA-Model: Train Data - \" + str(corpus_piece+1) + \" sub_corpus set\" + 15*\"#\")\n",
    "    ###### check the prediction of your lda\n",
    "    predict_stack = [topic.split(\"+\")[0].replace('\"',\"\").replace(\" \",\"\") for topic in topic_predict(test_buf.iloc[test_pivot], ldamodel_self, dictionary)]\n",
    "    figure_out = [figure_out for figure_out in predict_stack if figure_out in label_token]    \n",
    "    print(\"The match topic:\")\n",
    "    print(str(figure_out))\n",
    "    print(\"The topic set: \"+ str(predict_stack))\n",
    "    print(topic_predict(test_buf.iloc[test_pivot], ldamodel_self, dictionary))\n",
    "    print(90*\"-\")\n",
    "\n",
    "### Monitor\n",
    "print(15*\"#\" + \" LDA-Model: Train Data - \" + str(num_piece) + \" sub_corpus set\" + 15*\"#\")\n",
    "predict_stack = [topic.split(\"+\")[0].replace('\"',\"\").replace(\" \",\"\") for topic in topic_predict(test_buf.iloc[test_pivot], ldamodel, dictionary)]\n",
    "figure_out = [figure_out for figure_out in predict_stack if figure_out in label_token]    \n",
    "print(\"The match topic:\")\n",
    "print(str(figure_out))\n",
    "print(\"The topic set: \"+ str(predict_stack))\n",
    "print(topic_predict(test_buf.iloc[test_pivot], ldamodel, dictionary))\n",
    "print(90*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
