{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim, os, pickle\n",
    "import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### take some data\n",
    "df_train = pd.read_csv('../../../../Data/Raw/train_clean.csv',encoding='utf-8')\n",
    "qid_stack = df_train['qid1'].append(df_train['qid2'])\n",
    "df_train_naExc = df_train[~df_train.isnull().any(axis=1)]\n",
    "df_buf1 = df_train_naExc[['qid1','question1']]\n",
    "df_buf2 = df_train_naExc[['qid2','question2']]\n",
    "df_buf1.columns = ['qid','question']\n",
    "df_buf2.columns = ['qid','question']\n",
    "df_stack = df_buf1.append(df_buf2)\n",
    "df_stack_dupExc = df_stack.drop_duplicates()\n",
    "test_buf = df_stack_dupExc['question']#.head(1000)#.iloc[5000:7000]#head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_file = open('LDA_pickle_New_20170520_50topic_ldamodel_test_traing.pkl', 'rb')\n",
    "ldamodel_dump = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file1 = open('Py2_dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(pkl_file1)\n",
    "pkl_file.close()\n",
    "#print(ldamodel_dump.keys())\n",
    "##### for predict \n",
    "#ldamodel = ldamodel_dump['lda_model']\n",
    "#dictionary = ldamodel_dump['dictionary']\n",
    "##### for build the lda-model by yourself \n",
    "#corpus = ldamodel_dump['encoding_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# some tools\n",
    "######### Tokenize the corpus\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "def tokenize_doc(new_corpus):\n",
    "    new_corpus\n",
    "    texts = []\n",
    "    ###\n",
    "    ### Progressing bar setting\n",
    "    totall_size = len(new_corpus)\n",
    "    counter = 0\n",
    "    progress = 0\n",
    "    # loop through document list\n",
    "    for i in new_corpus:    \n",
    "#         ### Monitor the progress\n",
    "#         if progress%(1000) ==0:\n",
    "#             bar = \"[\" + int(progress/1000)*\"|\"+\">\"+ (10-int(progress/1000))*\"-\" + str((progress/100)) + \"%\" \"]\"\n",
    "#             print(bar)\n",
    "#         #### count\n",
    "#         counter += 1\n",
    "#         progress = int(10000*(counter/totall_size))\n",
    "\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        \n",
    "        # stem tokens\n",
    "#         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        stemmed_tokens = []\n",
    "        for i in stopped_tokens:\n",
    "            try:\n",
    "                stemmed_tokens = stemmed_tokens + [p_stemmer.stem(i)]\n",
    "            except:\n",
    "                print(i)\n",
    "        # add tokens to list            \n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "######  predict the topic\n",
    "def topic_predict(new_corpus, model, dictionary):\n",
    "    tokenize_buf = tokenize_doc([new_corpus])\n",
    "    doc_bow = [dictionary.doc2bow(text) for text in tokenize_buf]\n",
    "    que_vec = [item for itemList in doc_bow for item in itemList]\n",
    "    topic_vec = model[que_vec]\n",
    "\n",
    "    word_count_array = np.empty((len(topic_vec), 2), dtype = np.object)\n",
    "    for i in range(len(topic_vec)):\n",
    "        word_count_array[i, 0] = topic_vec[i][0]\n",
    "        word_count_array[i, 1] = topic_vec[i][1]\n",
    "\n",
    "    idx = np.argsort(word_count_array[:, 1])\n",
    "    idx = idx[::-1]\n",
    "    word_count_array = word_count_array[idx]\n",
    "\n",
    "    final = []\n",
    "    final = model.print_topic(20, 20)# model.print_topic(word_count_array[0, 0], len(word_count_array)) # model.print_topic(word_count_array[0, 0], len(word_count_array))#1)\n",
    "\n",
    "    question_topic = final.split('*') ## as format is like \"probability * topic\"\n",
    "    return question_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27, u'0.231*\"peopl\" + 0.104*\"think\" + 0.050*\"import\" + 0.034*\"american\"'), (5, u'0.067*\"state\" + 0.046*\"china\" + 0.036*\"unit\" + 0.027*\"polit\"'), (28, u'0.066*\"math\" + 0.064*\"2\" + 0.061*\"c\" + 0.055*\"market\"'), (34, u'0.054*\"man\" + 0.039*\"choos\" + 0.037*\"purpos\" + 0.035*\"girlfriend\"'), (8, u'0.053*\"cultur\" + 0.040*\"differ\" + 0.032*\"group\" + 0.029*\"convert\"'), (17, u'0.047*\"trump\" + 0.047*\"2016\" + 0.045*\"come\" + 0.043*\"product\"'), (7, u'0.096*\"phone\" + 0.039*\"social\" + 0.030*\"histori\" + 0.027*\"price\"'), (23, u'0.200*\"mean\" + 0.094*\"say\" + 0.046*\"die\" + 0.041*\"futur\"'), (31, u'0.072*\"sex\" + 0.063*\"data\" + 0.038*\"law\" + 0.035*\"big\"'), (13, u'0.159*\"life\" + 0.104*\"movi\" + 0.075*\"chang\" + 0.040*\"watch\"')]\n"
     ]
    }
   ],
   "source": [
    "###### check the topic stack\n",
    "print(ldamodel_dump.print_topics(num_topics=10, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Predict a topic\n",
    "pivot = 100\n",
    "print(test_buf.iloc[pivot])\n",
    "print(topic_predict(test_buf.iloc[pivot], ldamodel_dump, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Question:\n",
      "Will there really be any war between India and Pakistan over the Uri attack  What will be its effects \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "What is the easiest way to become a billionaire($) \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "How can we acquire a positive morality \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "Why do dreams look so real \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "What makes a programmer \"good \"\n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "What should I do for Web design \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "What is the difference between a turkey and a chicken \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "What is the benefit of going Walking every morning \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n",
      "Question:\n",
      "Prove that SNR of power = (SNR of voltage) sequare \n",
      "\n",
      "[u'0.127', u'\"t\" + 0.065', u'\"want\" + 0.053', u'\"feel\" + 0.051', u'\"love\" + 0.047', u'\"don\" + 0.041', u'\"friend\" + 0.028', u'\"can\" + 0.028', u'\"like\" + 0.021', u'\"now\" + 0.021', u'\"know\" + 0.021', u'\"tell\" + 0.020', u'\"doesn\" + 0.020', u'\"even\" + 0.016', u'\"relationship\" + 0.015', u'\"talk\" + 0.014', u'\"get\" + 0.014', u'\"tri\" + 0.013', u'\"marri\" + 0.012', u'\"someth\" + 0.012', u'\"alway\"']\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "###### Predict a topic\n",
    "print(\"=\"*90)\n",
    "for i in range(100,1000,100):\n",
    "    pivot = i\n",
    "    print(\"Question:\\n\"+test_buf.iloc[pivot]+\"\\n\")\n",
    "    print(topic_predict(test_buf.iloc[pivot], ldamodel_dump, dictionary))\n",
    "    print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
